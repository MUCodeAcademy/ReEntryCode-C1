# Answer Key

## Section 1

1. B  
2. B  
3. B  
4. C  
5. C  

## Section 2

6. True  
7. True  
8. False  
9. False  
10. True  

## Section 3

11. A model output that is fluent but not grounded in fact - i.e., a plausible-sounding statement that isn't true or supported by its training data.  
12. A pretraining process where the model learns to predict parts of its own input (e.g. next tokens) using no external labels.  
13. A technique that, at inference time, fetches relevant documents from an external database or index and conditions the model on that retrieved context.  

## Section 4

14. **embedding**  
15. **softmax**  
16. **attention/multi-attention/self-attention**, **feed-forward**  

## Section 5

A–2, B–1, C–4, D–3  